\documentclass[12pt]{article}%
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}%
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{indentfirst}
\setcounter{MaxMatrixCols}{30}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\begin{document}
\SweaveOpts{concordance=TRUE}
\title{Projekt MOW - Dokumentacja końcowa}
\author{Maciej Poćwierz\\
Piotr Chmielewski\\
Jacek Bylina
}
\date{\today}
\maketitle

<<echo = F>>=
require(caret)
require(ggplot2)
require(dplyr)
plot_conf_matrix <- function(fit){
  tab <- confusionMatrix(fit)
  tab <- tab$table/colSums(tab$table)
  
  confusion <- as.data.frame(tab)
  
  plot <- ggplot(confusion) +
    geom_tile(aes(x=Reference, y=Prediction, fill=Freq)) +
    geom_text(aes(x=Reference, y=Prediction, label = round(Freq, 2))) +
    scale_x_discrete(name="Actual Class") +
    scale_y_discrete(name="Predicted Class")+
    scale_fill_gradient(breaks=seq(from=0, to=1, by=.2)) +
    labs(fill="Normalized\nFrequency") +
    theme(legend.text=element_text(size=15), legend.position="bottom", legend.key.width=unit(1.5,"cm"))
  print(plot)
}
@

\section{Szczegółowa interpretacja tematu projektu}

\subsection{Opis danych}

\subsection{Naiwny klasyfikator bayesowski}

Zbiór danych podzielono na zbiór trenujący składający się z  80\% przykładów i testowy złożony z 20\% przykładów. Przy podziale zachowano procentowy rozkład klas przykładów. Wyliczanie prawdopodobieństw a-posteriori w regule Bayesa wymagało przekształcenie wielu atrybutów binarnych opisujących tę samą cechę przykładu na pojedynczy atrybut dyskretny. Zestaw atrybutów binarnych Wilderness\_Area\_*, zamieniono na atrybut dyskretny przyjmujący wartości ze zbioru \{1, 2, 3\}. Podobnie zestaw atrybutów binarnych opisujący typ gleby przekształcono w pojedynczy atrybut o przciwdziedzinie \{1, 2, \ldots{} , 40\}. Naiwny klasyfikator Bayesa estymuje prawdopodobieństwo $P(x_j|C_i)$ dla atrybutów ciągłych funkcją gęstości zgodnie z przyjętym rozkładem wartości atrybutów. W ramach projektu estymowano to prawdopodobieństwo funkcją gęstości Gauss'a oraz Laplace'a otrzymując bardzo zbliżoną skuteczność klasyfikacji. Jakość klasyfikacji na wydzielonym zbiorze testowym wyniosła ok. 66\%. 



\subsection{Klasyfikator K najbliższych sąsiadów}
<<echo = F>>=
load("knn_selected_fit.Rdata")
load("knn_raw_fit.Rdata")
load("knn_range_fit.Rdata")
load("knn_normalize_fit.Rdata")
@
Pierwszym krokiem konstruowania klasyfikatora kNN był podział zbioru trenującego na osobne zbiory uczący i testowy, mając na uwadze, aby podział ten nie zaburzył częstości występowania poszczególnych klas w obu zbiorach.
Następnie, usunięto z obu zbiorów kolumny reprezentujące atrybuty dyskretne, tzn. kolumny Soil\_Type\_* oraz Wilderness\_*.
Zaobserwowano, że jakość tak konstruowanego klasyfikatora jest największa dla parametru k=1.

\begin{center}
\begin{tabular}{ | p{1.5cm} |  p{1.5cm} | p{1.5cm}| p{1.5cm} |  p{1.5cm} |} 
\hline
k & 1 & 2 & 3 & 4 \\ 
\hline
quality & 84,0\% & 80,8\% & 81,5\% & 79,8\% \\ 
\hline
\end{tabular}
\end{center}

Pierwszym z wypróbowanych usprawnień była normalizacja zbioru danych. Wbrew przypuszczeniom, zastosowanie tego zabiegu nie przyniosła poprawy jakości klasyfikacji.

\begin{center}
\begin{tabular}{ | p{1.5cm} |  p{1.5cm} | p{1.5cm}| p{1.5cm} |  p{1.5cm} |} 
\hline
k & 1 & 2 & 3 & 4 \\ 
\hline
quality & 76,1\% & 72,8\% & 74,4\% & 73,4\% \\ 
\hline
\end{tabular}
\end{center}

Aby zrozumieć takie zachowanie klasyfikatora trzeba uwzględnić fakt, że jeśli dane nie są znormalizowane, to atrybuty o bardziej zróżnicowanych wartościach będą miały większy wpływ na wynik klasyfikacji, niż atrybuty o wartościach zbliżonych. Przykładowo dla analizowanych danych, atrybut Elevation o wartościach z zakresu 5000 - 12000, będzie miał większy wpływ na wynik klasyfikacji niż atrybut Slope o wartościach z zakresu 0 - 60, bez względu na to który z nich mocniej koreluje ze zmienną zależną.
Klasyfikator działał lepiej przed normalizacją, ponieważ w analizowanym zbiorze, atrybuty o dużym rozrzucie wartości wykazują dużą korelację ze zmienną zależną.
Mając powyższe na uwadze, podjęto próbę poprawy jakości klasyfikacji, poprzez przemnażanie wartości odpowiednich atrybutów przez odpowiednie wagi. Jako wartości wag wykorzystano wartości indeksu Giniego, obliczone za pomocą funkcji randomForest.

\begin{center}
\begin{tabular}{ | p{7cm} |  p{4cm} |} 
\hline

 & MeanDecreaseGini \\ \hline
Elevation & 1733.583623 \\ \hline
Aspect & 268.802532 \\ \hline
Slope & 192.8704514 \\ \hline
Horizontal\_Distance\_To\_Hydrology & 383.3969571 \\ \hline
Vertical\_Distance\_To\_Hydrology & 301.9205956 \\ \hline
Horizontal\_Distance\_To\_Roadways & 588.8973193 \\ \hline
Hillshade\_9am & 302.8200518 \\ \hline
Hillshade\_Noon & 245.6645633 \\ \hline
Hillshade\_3pm & 257.5732369 \\ \hline
Horizontal\_Distance\_To\_Fire\_Points & 425.5022385 \\ \hline
\end{tabular}
\end{center}



Udało się w ten sposób uzyskać pewną poprawę wyników, jednak nie uzyskano wyników lepszych niż przed normalizacją.
\begin{center}
\begin{tabular}{ | p{1.5cm} |  p{1.5cm} | p{1.5cm}| p{1.5cm} |  p{1.5cm} |} 
\hline
k & 1 & 2 & 3 & 4 \\ 
\hline
quality & 83,0\% & 80,0\% & 81,7\% & 80,7\% \\ 
\hline
\end{tabular}
\end{center}



Knn \\

<<fig = T, echo = F>>=
knn_selected_fit$results$model <- 'selected' 
knn_raw_fit$results$model <- 'raw' 
knn_range_fit$results$model <- 'range' 
knn_normalize_fit$results$model <- 'scale' 

results <- union(knn_selected_fit$results, knn_raw_fit$results)
results <- union(results, knn_range_fit$results)
results <- union(results, knn_normalize_fit$results)

results$model <- as.factor(results$model)
res_plot <- ggplot(data = results,
                 aes(x=k, y=Accuracy, group= model, colour= model)) +
            geom_line() +
            theme(legend.text=element_text(size=15),
                  legend.position="bottom") +
            scale_x_continuous(breaks = seq(min(results$k), max(results$k), by = 1)) +
            geom_point()
print(res_plot)
@


<<fig = T, echo = F>>=
plot_conf_matrix(knn_raw_fit)
@


C50
<<echo = F>>=

# C50 load
require(ggplot2)
require(caret)
load("rpart_fit.Rdata")
@


<<fig = T, echo = F>>=

# C50 confusion matrix
plot_conf_matrix(rpart_fit)
@


<<fig = T, echo = F>>=

# C50 parameters modification
res_plot <- ggplot(rpart_fit) +
  theme(legend.text=element_text(size=15),
        legend.position="bottom")
print(res_plot)
@



Random Forest
<<echo = F>>=

# Random Forest load
require(randomForest)
require(ggplot2)
load("rf_fit.Rdata")
load("rf_var_importance.Rdata")
@


<<fig = T, echo = F>>=

# Random Forest confusion matrix
plot_conf_matrix(rf_fit)
@


<<fig = T, echo = F>>=

# Random Forest parameters modification
plot(rf_fit)
@


<<fig = T, echo = F>>=

# Random Forest - final model error rate by classes
layout(matrix(c(1,2),nrow=1),width=c(4,1)) 
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rf_fit$finalModel, col=1:8, main = "Final model", log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(rf_fit$finalModel$err.rate),col=1:8,cex=0.8,fill=1:8)
@

<<fig = T, echo = F>>=

# Random Forest - var importance
varImpPlot(rf_var_importance, type = 1, n.var = 20)
@


<<fig = T, echo = F>>=

# Random Forest - var importance
varImpPlot(rf_var_importance, type = 2, n.var = 20)
@


Bayes \\
<<echo = F>>=

# Bayes load
require(dplyr)
require(ggplot2)
load("e1071_fit.Rdata")
load("e1071_fit_2.Rdata")
load("klar_fit.Rdata")
@



<<fig = T, echo = F>>=

# Bayes confusion matrix
plot_conf_matrix(klar_fit)
@

<<fig = T, echo = F>>=

# Bayes compare models
e1071_fit$results$model <- 'e1071_fit'
e1071_fit$results$usekernel <- FALSE
e1071_fit_2$results$model <- 'e1071_fit_2'
e1071_fit_2$results$usekernel <- FALSE

klar_fit$results$model <- paste('klar_fit', 'usekernel', klar_fit$results$usekernel)
names(klar_fit$results)[names(klar_fit$results)=="fL"] <- "laplace"


union_fit <- union(e1071_fit$results, e1071_fit_2$results)
union_fit <- bind_rows(union_fit, klar_fit$results)
union_fit$model <- as.factor(union_fit$model)

bayes_plot <- ggplot(data = union_fit, aes(x=laplace, y=Accuracy, group= model, colour= model)) +
  geom_line() +
  theme(legend.text=element_text(size=10),
        legend.position="bottom") +
  scale_x_continuous(breaks = seq(min(union_fit$laplace), max(union_fit$laplace), by = 0.5)) +
  geom_point()

print(bayes_plot)
@


\end{document}